{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7e5389",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/04_Classification'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92f635",
   "metadata": {},
   "source": [
    "# More Classifiers, Evaluation Methods & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcff672",
   "metadata": {},
   "source": [
    "In this exercise, we will use the **Iris dataset**, which you can find in **data/iris.csv**. \n",
    "\n",
    "The dataset describes three types of Iris flowers:\n",
    "- Setosa\n",
    "- Virginica\n",
    "- Versicolour\n",
    "\n",
    "There are four (non-class) attributes\n",
    "- Sepal width and length\n",
    "- Petal width and length\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/iris_dataset_meme.png\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# load the data\n",
    "iris = pd.read_csv(\"data/iris.csv\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training features and target variable\n",
    "iris_data = iris[['SepalLength','SepalWidth','PetalLength','PetalWidth']]\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "iris_target = label_encoder.fit_transform(iris['Name'])\n",
    "\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3d21a",
   "metadata": {},
   "source": [
    "# Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94cb1f",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Fundamental theorem in probability that describes how to update our belief about an event based on new evidence.\n",
    "- It computes the **conditional probability $P(C|A)$** that tells us the probability of a class C given some attribute A\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/bayes_theorem.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "- **P(C|A)** is the **posterior / conditional probability**: the probability of class $C$ _after_ attribute $A$ is seen  \n",
    "- **P(A|C)** is the **likelihood / class-conditional probability**: the probability of observing attribute $A$ given class $C$ \n",
    "- **P(C)**: is the **prior probability of class C**: the initial probability of class $C$ _before_ attributes are seen\n",
    "- **P(A)** is the **marginal probability**: the total probability of attribute $A$ across all possible classes\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/baes_theorem.jpeg\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df63ef",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes Classifier is a simple classification algorithm based on Bayes' Theorem.\n",
    "\n",
    "Let's classify whether an email is **Spam** or **Not Spam**. We have a dataset of **5 emails** with the words **\"Offer\"** and **\"Free\"** and their corresponding class labels.\n",
    "\n",
    "| Email | Word: \"Offer\" | Word: \"Free\" | Class (Spam/Not Spam) |\n",
    "|--------|------------|------------|--------------------|\n",
    "| 1      | Yes        | Yes        | Spam              |\n",
    "| 2      | Yes        | No         | Spam              |\n",
    "| 3      | No         | Yes        | Spam              |\n",
    "| 4      | Yes        | Yes        | Not Spam          |\n",
    "| 5      | No         | No         | Not Spam          |\n",
    "\n",
    "üìù **Feature Representation:**  \n",
    "- **Yes (1)** means the word is present in the email.  \n",
    "- **No (0)** means the word is absent.\n",
    "\n",
    "\n",
    "### How does Naive Bayes Work?\n",
    "\n",
    "#### 1. Compute the prior probabilites $P(C_j)$\n",
    "- For each class $C_j$, count the records in the training set that are labeled with class $C_j$ and divide the count by the overall number of records\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "    \n",
    "#### 2. Estimate the class-conditional probability $P(A|C)$\n",
    "   \n",
    "   - ‚ö†Ô∏è Naive Bayes **assumes** that all **features** are **conditionally independent** (**Naive Bayes assumption**)\n",
    "   \n",
    "   - **Important**: this independence assumption is almost never correct!\n",
    "   \n",
    "   - ‚úÖ Thanks to the _independence assumption_, we can re-write the joint probabiity $P(A|C)$ as the product of the invididual probabilities $P(A_i|C_j)$ (which we can estimate directly from the training data for all $A_i$ and $C_j$):\n",
    "   \n",
    "   $P(A_1, A_2, ..., A_n|C_j) = P(A_1|C_j) \\times P(A_2|C_j) \\times ... \\times P(A_n|C_j) = \\prod_{i=1}^n P(A_i|C_j)$\n",
    "   \n",
    "   - **In practice**: Estimate  $P(A_i|C_j)$ by counting how often an attribute value co-occurs with class $C_j$, and divide by the overall number of examples belonging to class $C_j$\n",
    "   \n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "\n",
    "#### 3. Apply Bayes' Theorem\n",
    "\n",
    "- The probability of a sample $A$ belonging to class $C_j$ is: $P(C_j|A) = \\frac{P(A|C_j)P(C_j)}{P(A)}$\n",
    "- Since **$P(A)$** is the **same for all classes**, we can compare probabilities using $P(C_j|A) \\propto P(C_j) \\prod_{i=1}^n P(A_i|C_j)$\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "Suppose we receive a **new email**:  üìß **\"Offer Free\"** (contains both words \"Offer\" and \"Free\")\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "\n",
    "#### 4. Classification Decision\n",
    "- Assign A to the class that **maximizes** the posterior probability, i.e., the class with the highest probability: $\\hat{C} = arg max_{C_j} P(C_j) \\prod_{i=1}^n P(A_i|C_j)$\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "---------------------------------------------------------------------------------------------------------   \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "10ff7b015ba47491"
  },
  {
   "cell_type": "markdown",
   "id": "d2689c43",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è  Zero-Frequency Problem\n",
    "\n",
    "This problem occurs in Naive Bayes classification when a feature value **never appears** in the training set for a particular class. This leads to a **zero class-conditional probability**, which causes problems when computing the final probability using Bayes' theorem.\n",
    "\n",
    "#### Why Is This a Problem?\n",
    "If any feature $A_i$ has $P(A_i|C_j)=0$, then the entire product becomes **zero**, making it impossible to classify the instance correctly.\n",
    "\n",
    "#### Solution: Laplace Smoothing\n",
    "\n",
    "Add a small constant $\\alpha$ (i.e., usually 1) to each probability estimate\n",
    "    \n",
    "- Original: $P(A_i|C_j) = \\frac{N_{ic}}{N_c}$\n",
    "- Laplace: $P(A_i|C_j) = \\frac{N_{ic} + 1}{N_c + c}$\n",
    "\n",
    "where $c$ = number of attribute values of $A$\n",
    "\n",
    "‚úÖ Probabilities will never be zero!\n",
    "\n",
    "‚úÖ Stabilizes probability estimates\n",
    "\n",
    "## Strengths\n",
    "- Works very well, even is the independence assumption is violated\n",
    "- **Robust** to **isolated noise points**, as they will be averaged out\n",
    "- **Robust** to **irellevant attributes**, as $P(A_i|C_j)$ is distributed uniformly for $A_i$\n",
    "- **Computationally cheap**: probabilities can be estimated doing one pass over the training data\n",
    "- **Memory efficient**: storing the probabilities does not require a lot of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1604e1",
   "metadata": {},
   "source": [
    "## Naive Bayes in Scikit-learn\n",
    "\n",
    "[Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) is implemented in different variations in scikit-learn.\n",
    "They differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$\n",
    "\n",
    "\n",
    "- [```GaussianNB``` class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Naive Bayes classifier for continious (numeric) features. Likelihood of the features is assumed to be Gaussian\n",
    "- [```MultinomialNB``` class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) implements the Naive Bayes classifier for discrete (categorical) features (multinomially distributed data)\n",
    "- [```BernoulliNB``` class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) assumes multivariate Bernoulli distributions\n",
    "- [```CategoricalNB``` class](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) assumes that each feature has its own categorical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_bayes = GaussianNB()\n",
    "naive_bayes.fit(iris_data, iris_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90de2cf",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ff91a",
   "metadata": {},
   "source": [
    "## What is SVM?\n",
    "\n",
    "It's a machine learning algorithm used for classification and regression. It classifies data by finding an optimal line or hyperplane that maximizes the distance between each class in an N-dimensional space.\n",
    "\n",
    "## How does SVM work?\n",
    "\n",
    "Find a linear hyperplance (decision boundary) that **maximizes** the margin to the closest points (support vectors).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/svm_1.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    " ‚ö†Ô∏è If the **decision boundary is not linear**, then transform the data into a higher dimensional space using a **Kernel function**.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/svm_2.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Strenghts\n",
    "\n",
    "- Works well in **high dimensional spaces** (i.e., many features)\n",
    "- **Memory efficient**: it uses a subset of training points in the decision function (i.e., suppprt vectors)\n",
    "- **Versatile**: different Kernel functions can be specified for the decision function\n",
    "- Can handle **non-linear data** using the kernel trick.\n",
    "\n",
    "\n",
    "\n",
    "## Limitations\n",
    "- **Computationaly expensive** on large datasets, especially when using complex kernels\n",
    "- **Difficult to choose the right kernel**: the choice of kernel (e.g., linear, polynomial, RBF) is crucial and requires hyperparameter optimization\n",
    "- **Hard to interpret**: the decision boundary is abstract and hard to interpret, especially in high-dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6be1de",
   "metadata": {},
   "source": [
    "## SVM in Python\n",
    "\n",
    "[Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html) are also implemented in different variations.\n",
    "\n",
    "We will be using the [```SVC``` class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) which implements support vector classification.\n",
    "An alternative implementation with different parameters is the [```NuSVC``` class](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(iris_data, iris_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015bf4d",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c666b",
   "metadata": {},
   "source": [
    "## Perceptron: The Simplest Neural Unit\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/ann_1.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "## Multi-layer ANNs\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/ann_2.jpg\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "### Training\n",
    "1. Initialize the weights ($w_0$, $w_1$, ..., $w_n$), either randomly or using pretrained weights\n",
    "2. Adjust the weights such that the output of the ANN is as consistent as possible with the class labels of the training examples:\n",
    "    - Using an **objective function**, e.g. $E = \\sum_i [Y_i - f(w_i, X_i)]^2$\n",
    "    - Find the weights $w_i$ that minimize $E$ using **backpropagation**\n",
    "    - Adjustment factor: **learning rate**\n",
    "    \n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"imgs/backprop.png\" style=\"width: 60%;\">\n",
    "    </div>\n",
    "    \n",
    "\n",
    "### Differences compared to the perceptron\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/ann_3.png\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a901e",
   "metadata": {},
   "source": [
    "# Evaluation Methods\n",
    "\n",
    "**Goal**: Obtain a reliable estimate of the model's gneralization performance\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/evaluation_meme.jpg\" style=\"width: 40%;\">\n",
    "</div>\n",
    "\n",
    "## ‚ö†Ô∏è NEVER EVER TEST A MODEL ON DATA THAT WAS USED FOR TRAINING!!‚ö†Ô∏è \n",
    "\n",
    "**General approach**: split the labeled records into a training set and a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e98746",
   "metadata": {},
   "source": [
    "## Holdout Method\n",
    "\n",
    "This methout reserves a certain amount of the labeled data for testing, and uses the remainder for training.\n",
    "- Applied when **lots of sample data** is available\n",
    "- Typical train / test splits: 75% / 25% or 80% / 20%\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/holdout_method.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "‚ö†Ô∏è Random samples might not be representative for imbalanced datasets, as few or no records o the minority class will be in the training or test sets\n",
    "\n",
    "- **Stratified Sampling**: Sample each class independently, so that records of the minority clss are present in each sample\n",
    "- **Random Subsampling**: Repeat the process with different subsamples, i.e., in each iteration, a certain proportion is randomly selected for training and the performance of the different iterations is averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a10e9c",
   "metadata": {},
   "source": [
    "## Leave One Out Method\n",
    "\n",
    "It iterates over all examples as follows:\n",
    "- Train a model on all examples but the current one\n",
    "- Evaluate on the current example\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/leave_one_out_method.jpg\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "‚úÖ Produces very accurate estimates\n",
    "\n",
    "‚ùå Computationally infeasible "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94029335",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "**K-fold cross-validation**:\n",
    "- Splits the data into **k equally sized subsets** (usually $k=10$ and stratified sampling is used)\n",
    "- Each subset in turn is used for testing, and the remainder for training\n",
    "- The error estimates are averaged over all subsets to yield the overall error estimate\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/cross_validation.png\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e87b6",
   "metadata": {},
   "source": [
    "### Cross-Validation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "accuracy_iris = cross_val_score(dt, iris_data, iris_target, cv=10, scoring='accuracy')\n",
    "\n",
    "for i, acc in enumerate(accuracy_iris):\n",
    "    print(\"Fold {}: Accuracy = {}%\".format(i, acc * 100.0))\n",
    "\n",
    "print(\"Average Accuracy = {}%\".format(accuracy_iris.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da596d",
   "metadata": {},
   "source": [
    "### Stratified Sampling in Cross Validation\n",
    "\n",
    "You can control how the folds are created by changing the ```cv``` parameter.\n",
    "Stratified sampling is implemented in the [```StatifiedKFold``` class](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_iris = cross_val_score(dt, iris_data, iris_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_iris.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c680df8",
   "metadata": {},
   "source": [
    "### Obtaining predictions by cross-validation\n",
    "\n",
    "If you want to analyse the predictions made during cross validation (for error analysis, you don't apply cross validation when actually applying the model!), you can use the [```cross_val_predict()``` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html).\n",
    "\n",
    "**Note**: As the folds of a cross validation are non-overlapping, you get exactly one prediction for every example in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted = cross_val_predict(dt, iris_data, iris_target, cv=10)\n",
    "\n",
    "display(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26815444",
   "metadata": {},
   "source": [
    "### Manual Cross Validation \n",
    "If you want to implement cross validation yourself, you can iterate over the folds manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025be8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes you have to use the raw array and not the pandas dataframe (access it with .values)\n",
    "data = iris_data.values \n",
    "target = iris['Name']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for train_indices, test_indices in cv.split(data, target):\n",
    "    train_data = data[train_indices]\n",
    "    train_target = target[train_indices]\n",
    "    \n",
    "    dt.fit(train_data, train_target)\n",
    "\n",
    "    test_data = data[test_indices]\n",
    "    test_target = target[test_indices]\n",
    "    \n",
    "    test_prediction = dt.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a4d50",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "A [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in scikit-learn allows you to specify a sequence of transforms and a final estimator that can be executed and cross-validated.\n",
    "This way you don't have to worry about applying the preprocessing steps (transforms) properly to each training and test split.\n",
    "\n",
    "You create a pipeline by defining the steps that should be executed as a list.\n",
    "Each element of the list is a tuple that consists of a name and the transform or estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb27199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "transform = StandardScaler()\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "pipeline = Pipeline([ ('normalisation', transform), ('classification', estimator) ])\n",
    "\n",
    "accuracy_iris = cross_val_score(pipeline, iris_data, iris_target, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"Average Accuracy = {}%\".format(accuracy_iris.mean() * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c91cf1",
   "metadata": {},
   "source": [
    "# Intermezzo: Hyperparameter Optimization\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/hpo_meme.jpg\" style=\"width: 50%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Hyperparameter Selection\n",
    "\n",
    "**Hyperparameter**: a parameter which influences the learning process and whose value is **set before the learning begins** (e.g., learning rate, number of hidden layers for ANNs, pruning thresholds for decision trees, $K$ for K-NN)\n",
    "\n",
    "**Parameter**: the values learned by an estimator during training / from the training data (e.g., weights in ANN, splits in a tree)\n",
    "\n",
    "### üõ†  The complete learning procedure is thus:\n",
    "- Hyperparameter Tuning ‚û°Ô∏è pick best hyperparameters\n",
    "- Training ‚û°Ô∏è find best parameters\n",
    "- Testing model performance on *unseen* test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00763db8",
   "metadata": {},
   "source": [
    "**Goal of Hyperparameter Optimization**: find the combination of hyperparameter values that result in learning the model with the lowest generalization error\n",
    "\n",
    "## Search Strategies\n",
    "\n",
    "### 1. Brute Force Search\n",
    "- Try out all hyperparameter combinations \n",
    "- Computationally impossible; ‚Äúblind‚Äù evaluation of parameters\n",
    "\n",
    "### 2. Grid Search\n",
    "- Manually restrict search space to certain parameter combinations\n",
    "- Quality of solution strongly dependent on grid definition\n",
    "- It may miss the best parameters\n",
    "\n",
    "\n",
    "### 3. Random Search\n",
    "- Test all combinations of random parameter values\n",
    "\n",
    "\n",
    "### 4. Bayesian Optimization\n",
    "- Treat hyperparameter tuning as a learning problem:\n",
    "    - Given a set of hyperparameters $p$, predict the evaluation score $s$ of the model\n",
    "    - The prediction model is called a **surrogate model** or **oracle**\n",
    "- Why? Because training and evaluating the actual model is costly\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/bayesian_optimization.png\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1daf9",
   "metadata": {},
   "source": [
    "### Grid Search in Python\n",
    "\n",
    "- We perform the hyper-parameter tuning using [Grid Search](http://scikit-learn.org/stable/modules/grid_search.html).\n",
    "- It is implemented in the [```GridSearchCV``` class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) in scikit-learn.\n",
    "- This class behaves exactly like an estimator. If its ```fit()``` function is called, all hyper-parameter combinations are evaluated.\n",
    "\n",
    "Parameters:\n",
    "- ```estimator```: an estimator (e.g. a decision tree)\n",
    "- ```parameter_grid```: the parameters that should be evaluated as a dictionary\n",
    "    - the key is the name of the hyper-parameter\n",
    "    - the value is a list of possible values\n",
    "    - example: ```{'param_a':[1,2,3], 'param_b':[7,8,9] }```\n",
    "- ```scoring```: the metric that should be used to evaluate the parameter settings (can be 'accuracy' or other scores)\n",
    "- ```cv```: specifies how to perform cross validation (default: 3-fold cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create an estimator\n",
    "knn_estimator = KNeighborsClassifier()\n",
    "\n",
    "# specify the parameter grid\n",
    "parameters = {\n",
    "    'n_neighbors': range(2, 9)\n",
    "}\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(\n",
    "    knn_estimator, \n",
    "    parameters, \n",
    "    scoring='accuracy', \n",
    "    cv=stratified_10_fold_cv, \n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# run the grid search\n",
    "grid_search_estimator.fit(iris_data,iris_target)\n",
    "\n",
    "# print the results of all hyper-parameter combinations\n",
    "results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "display(results)\n",
    "    \n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(\n",
    "    grid_search_estimator.best_score_, grid_search_estimator.best_params_)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e07e9",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "**Goal**: Select the model $m_{best}$ from all learned models $M$ that is expected to generalize best to unseen records\n",
    "\n",
    "## ‚ö†Ô∏è Separate data for model selection from the data for model evaluation!\n",
    "\n",
    "Otherwise: \n",
    "- overfitting to test set\n",
    "- overly optimistic generalization error estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be91f90",
   "metadata": {},
   "source": [
    "## Model Selection using a Validation Set\n",
    "\n",
    "1. Split training set $D_{train}$ into validation set $D_{val}$ and training set $D_{tr}$\n",
    "2. Learn models $m_i$ on $D_{tr}$ using different hyperparameter value combinations $p_i$\n",
    "3. Select best parameter values$p_{best}$ by testing each model $m_i$ on the validation set $D_{val}$\n",
    "4. Learn the final model $m_{best}$ on complete $D_{train}$ using the parameter values $p_{best}$\n",
    "5. Evaluate $m_{best}$ on test set in order to get a unbiased estimate of its generalization performance\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/model_selection_val_set.png\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8644c28",
   "metadata": {},
   "source": [
    "## Model Selection using a Cross-Validation \n",
    "\n",
    "‚úÖ Make sure that all examples are used for validation once\n",
    "\n",
    "‚úÖ Use as much labeled data as possible for training\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/model_selection_crossval.png\" style=\"width: 70%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dea144",
   "metadata": {},
   "source": [
    "# Back to Evaluation Methods\n",
    "\n",
    "## Nested Cross-Validation\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/model_selection_nested_crossval.png\" style=\"width: 70%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e521077",
   "metadata": {},
   "source": [
    "### Nested Cross-Validation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a762b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# use only 5 folds here, as we only have 50 examples per class in the iris dataset!\n",
    "nested_cv_score = cross_val_score(grid_search_estimator, iris_data, iris_target, cv=5, scoring='accuracy')\n",
    "\n",
    "display(nested_cv_score.mean())\n",
    "\n",
    "grid_search_estimator.fit(iris_data,iris_target)\n",
    "display(grid_search_estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6024f",
   "metadata": {},
   "source": [
    "### Grid Search using Pipelines\n",
    "\n",
    "Often, we need preprocessing steps before we perform a grid search, or even want to optimise the hyper-parameters of our preprocessing steps.\n",
    "In these cases, we set up a pipeline and run the grid search on all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19965c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the pipeline\n",
    "transform = StandardScaler()\n",
    "estimator = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[ ('normalisation', transform), ('classification', estimator) ])\n",
    "\n",
    "\n",
    "# specify the parameter grid\n",
    "parameters = {\n",
    "    'normalisation__with_mean': [ True, False],\n",
    "    'normalisation__with_std': [ True, False],\n",
    "    'classification__n_neighbors': range(2, 9)\n",
    "}\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=10)\n",
    "\n",
    "accuracy_best = cross_val_score(grid_search_estimator, iris_data, iris_target, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(\"Accuracy = {}%\".format(accuracy_best.mean() * 100.0))\n",
    "\n",
    "grid_search_estimator.fit(iris_data, iris_target)\n",
    "display(grid_search_estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e06ce",
   "metadata": {},
   "source": [
    "# Comparing Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91915ecb",
   "metadata": {},
   "source": [
    "## 1. Confidence Intervals\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/confidence_intervals_1.png\" style=\"width: 70%;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/confidence_intervals_2.png\" style=\"width: 70%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"color:red\">Caution: only for sample size > 30.</span>\n",
    "\n",
    "With p% probability, $error_D$ is in $[error_s - y, error_s + y]$, with $y = z_N \\cdot \\sqrt{\\frac{error_s (1 -error_s)}{n}}$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/z_table.png\" style=\"width: 50%;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d0676",
   "metadata": {},
   "source": [
    "### Computing Confidence Intervals\n",
    "\n",
    "You are using a machine learning solution from company A. Recently, you were contacted by the Junior Vice President of company B and he offered you to switch to his solution. As a migration is very costly, you only want to switch if you can be at least 90% sure that the new solution is better. For such purposes, you have a dedicated test set with 420 examples where your current solution makes 105 errors. \n",
    "\n",
    "What is the highest number of errors that you accept for the new solution in order to switch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b96b30",
   "metadata": {},
   "source": [
    "## 2. Statistical Tests: Sign Test vs. Wilcoxon Signed-Rank Test\n",
    "\n",
    "Let's consider two classifiers, $ A $  and $  B$ , evaluated on 10 test instances. We record their accuracy (or any performance metric):\n",
    "\n",
    "| Instance | Model A Score | Model B Score | Difference ($d$) | Sign |\n",
    "|----------|--------------|--------------|----------------|------|\n",
    "| 1        | 0.90         | 0.80         | **+0.10**      | +    |\n",
    "| 2        | 0.88         | 0.75         | **+0.13**      | +    |\n",
    "| 3        | 0.85         | 0.85         | **0.00**       | Tie  |\n",
    "| 4        | 0.92         | 0.88         | **+0.04**      | +    |\n",
    "| 5        | 0.80         | 0.78         | **+0.02**      | +    |\n",
    "| 6        | 0.89         | 0.82         | **+0.07**      | +    |\n",
    "| 7        | 0.91         | 0.84         | **+0.07**      | +    |\n",
    "| 8        | 0.87         | 0.86         | **+0.01**      | +    |\n",
    "| 9        | 0.76         | 0.79         | **‚àí0.03**      | ‚àí    |\n",
    "| 10       | 0.93         | 0.85         | **+0.08**      | +    |\n",
    "| 11       | 0.95         | 0.88         | **+0.07**      | +    |\n",
    "| 12       | 0.89         | 0.82         | **+0.07**      | +    |\n",
    "\n",
    "- **+** means Model A outperformed Model B.  \n",
    "- **‚àí** means Model B outperformed Model A.  \n",
    "- **Ties are removed.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9e5c4",
   "metadata": {},
   "source": [
    "### Sign Test\n",
    "\n",
    "The **Sign Test** only considers the number of wins/losses. The null hypothesis ($H_0$) assumes that Model A and Model B are equally good, meaning that each instance is equally likely to favor either model ($p = 0.5$).\n",
    "\n",
    "**Step 1: Count the wins**\n",
    "\n",
    "Ignoring the tie (Instance 3):\n",
    "- **Model A wins:** $n_A = 10$ \n",
    "- **Model B wins:** $n_B = 1$  \n",
    "- **Ties:** $n_t = 1$  \n",
    "- **Total non-tied instances:** $n' = 11$ \n",
    "\n",
    "**Step 2: Find the critical value**\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/sign_test_table.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "Since Model A performs better than Model B in 11 cases, we **reject \\( H_0 \\)** ‚Üí **Model A is significantly better than Model B.** ‚úÖ  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4045e91",
   "metadata": {},
   "source": [
    "### Wilcoxon Signed-Rank Test\n",
    "\n",
    "The **Wilcoxon signed-rank test** considers both **signs** and **magnitude** of the differences.\n",
    "\n",
    "**Step 1: Rank results by _absolute differences_**\n",
    "- Ties are ignored\n",
    "- Equal ranks are averaged\n",
    "\n",
    "| Instance | Model A Score | Model B Score | Difference ($d$) | Absolute $d$ | Rank |\n",
    "|----------|--------------|--------------|----------------|----------------|------|\n",
    "| 1        | 0.90         | 0.80         | **+0.10**      | 0.10           | **10**  |\n",
    "| 2        | 0.88         | 0.75         | **+0.13**      | 0.13           | **11**  |\n",
    "| 3        | 0.85         | 0.85         | **0.00**       | 0.00           | **Tie**  |\n",
    "| 4        | 0.92         | 0.88         | **+0.04**      | 0.04           | **4**  |\n",
    "| 5        | 0.80         | 0.78         | **+0.02**      | 0.02           | **2**  |\n",
    "| 6        | 0.89         | 0.82         | **+0.07**      | 0.07           | **7**  |\n",
    "| 7        | 0.91         | 0.84         | **+0.07**      | 0.07           | **7**  |\n",
    "| 8        | 0.87         | 0.86         | **+0.01**      | 0.01           | **1**  |\n",
    "| 9        | 0.76         | 0.79         | **‚àí0.03**      | 0.03           | **3**  |\n",
    "| 10       | 0.93         | 0.85         | **+0.08**      | 0.08           | **9**  |\n",
    "| 11       | 0.95         | 0.88         | **+0.07**      | 0.07           | **7**  |\n",
    "| 12       | 0.89         | 0.82         | **+0.07**      | 0.07           | **7**  |\n",
    "\n",
    "**Step 2: Sum ranks by sign**\n",
    "- **Sum of positive ranks**: $W_+ = 10 + 11 + 4 + 2 + 7 + 7 + 1 + 9 + 7 + 7 = 65$\n",
    " \n",
    "- **Sum of negative ranks**: $W_- = 3$\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/wilcoxon_signed_test_table.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "**Step 3: Compute the test statistic \\( W \\)**\n",
    "\n",
    "Wilcoxon‚Äôs statistic is the smaller sum of ranks:  \n",
    "$W = \\min(W_+, W_-) = \\min(65, 3) = 3$\n",
    "\n",
    "**Step 4: Find the critical value**\n",
    "\n",
    "Since $W = 3 < 13$, **we reject $H_0$** ‚Üí **Model A is significantly better than Model B.** ‚úÖ  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb9baf",
   "metadata": {},
   "source": [
    "# QUIZ TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfe04e",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "You train a Na√Øve Bayes classifier for sentiment analysis on movie reviews. The model predicts **positive sentiment** for the review:\n",
    "\n",
    "_\"The movie was absolutely amazing, the plot was thrilling, but the acting was mediocre.\"_\n",
    "\n",
    "The words \"amazing\" and \"thrilling\" are associated with **positive** sentiment, while \"mediocre\" is linked to **negative** sentiment. Why might Na√Øve Bayes still classify this as **positive**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e247432",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Table below contains information about different biological species. Using the training data, create a Naive\n",
    "Bayes classification model and classify the following examples:\n",
    "- Dolphin <yes, no, yes, no>\n",
    "- Duck <no, yes, sometimes, yes>\n",
    "\n",
    "| Gives birth | Can fly | Lives in Water | Has Legs | Class       |\n",
    "| ----------- | ------- | -------------- | -------- | ----------- |\n",
    "| yes         | no      | no             | yes      | mammals     |\n",
    "| no          | no      | no             | no       | non-mammals |\n",
    "| no          | no      | yes            | no       | non-mammals |\n",
    "| yes         | no      | yes            | no       | mammals     |\n",
    "| no          | no      | sometimes      | yes      | non-mammals |\n",
    "| no          | no      | no             | yes      | non-mammals |\n",
    "| yes         | yes     | no             | yes      | mammals     |\n",
    "| no          | yes     | no             | yes      | non-mammals |\n",
    "| yes         | no      | no             | yes      | mammals     |\n",
    "| yes         | no      | yes            | no       | non-mammals |\n",
    "| no          | no      | sometimes      | yes      | non-mammals |\n",
    "| no          | no      | sometimes      | yes      | non-mammals |\n",
    "| yes         | no      | no             | yes      | mammals     |\n",
    "| no          | no      | yes            | no       | non-mammals |\n",
    "| no          | no      | sometimes      | yes      | non-mammals |\n",
    "| no          | no      | no             | yes      | non-mammals |\n",
    "| no          | no      | no             | yes      | mammals     |\n",
    "| no          | yes     | no             | yes      | non-mammals |\n",
    "| yes         | no      | yes            | no       | mammals     |\n",
    "| no          | yes     | no             | yes      | non-mammals |\n",
    "\n",
    "Steps:\n",
    "1. Compute the prior probability of each class\n",
    "2. Compute the class conditional probability of evidence (for each attribute)\n",
    "3. Classify the _Dolphin_ and _Duck_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce607884",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Suppose you are using the **holdout method** to evaluate a machine learning model. You split your dataset into **70% training** and **30% testing**. You then perform **feature selection** on the **entire dataset** before training the model.\n",
    "\n",
    "Why is this a problem?\n",
    "How would this affect the model‚Äôs performance on new, unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25c1cf",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "A researcher performs **10-fold cross-validation** for **hyperparameter tuning** and then trains the final model on the entire dataset. They **evaluate this final model using another round of 10-fold cross-validation on the same dataset**.\n",
    "\n",
    "- Why is this evaluation flawed?\n",
    "- How should the researcher evaluate the final model properly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de38af",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Cross-validation is often preferred over the holdout method since it provides a more reliable estimate of generalization performance.\n",
    "\n",
    "In what scenarios might the **holdout method** be preferable to **cross-validation**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b9c88",
   "metadata": {},
   "source": [
    "## Question / Task 6\n",
    "\n",
    "This exercise is about hyperparameter tuning. To get familiar with hyperparameter tuning in scikit-learn, refer to the respective [part in the documentation](https://scikit-learn.org/stable/modules/grid_search.html).\n",
    "\n",
    "We will use the data set of the Data Mining Cup 2006, which you can find in **data/dmc2006**. The task is to predict the attribute `gms_greater_avg` as precisely as possible. We will use the F1-measure of the class `1` as main performance metric.\n",
    "\n",
    "1. Data preparation.\n",
    "    - Import the data and create a 50:50 train-test split.\n",
    "    - Implement the `evaluate_estimators` function so that it returns precision, recall, and F1-measure of the class 1 on the test set for the classifiers given in `estimators`. Use the following `estimators`: {Naive Bayes, K-NN, SVC}\n",
    "\n",
    "2. Grid Search\n",
    "    - Run a grid search with the parameters given in `tune_params` with F1-measure as optimization objective. \n",
    "    \n",
    "    ```python\n",
    "    tune_params = {\n",
    "    \n",
    "        'K-NN': {\n",
    "            'n_neighbors': [1, 3, 5, 10]\n",
    "        },\n",
    "        \n",
    "        'SVC': {\n",
    "            'C': [.001, .01, .1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'tol': [1e-2, 1e-3, 1e-4],\n",
    "            'class_weight': ['balanced', None],\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    - For the best estimator, print the parameters and evaluate it with the `evaluate_estimators` function.\n",
    "    \n",
    "    **HINT**: Take a look at https://scikit-learn.org/stable/modules/grid_search.html for infos about grid search.\n",
    "    \n",
    "3. Bayesian Optimization\n",
    "    - Now run a bayesian search with the parameters given in `bayes_tune_params` with F1-measure as objective. Use a `n_iter` of 15.\n",
    "    - Again, print parameters of the best estimator and evaluate it with the `evaluate_estimators` function.\n",
    "\n",
    "    **HINT**: Use scikit-optimize for bayesian search (https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html)\n",
    "\n",
    "    **HINT**: Currently, BayesSearchCV does not work with scikit-learn version of 0.24.1. Use version 0.23.2 instead      -> run a cell with `!pip install scikit-learn==0.23.2` and restart the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
