{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/06_Regression'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Audio, HTML\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"data/questions.json\", \"r\") as file:\n",
    "    questions = json.load(file)\n",
    "\n",
    "random.shuffle(questions)\n",
    "\n",
    "# Lifelines\n",
    "lifelines = {\"50:50\": 2, \"Buddy\": 2}\n",
    "\n",
    "# Game State\n",
    "current_question = 0\n",
    "score = 0\n",
    "lifeline_used = {\"50:50\": False, \"Buddy\": False}  # Track if a lifeline has been used this round\n",
    "\n",
    "output = widgets.Output()\n",
    "progress_label = widgets.Label()\n",
    "question_box = widgets.VBox([])\n",
    "lifeline_box = widgets.VBox([])\n",
    "next_button = widgets.Button(description=\"Next Question\", layout=widgets.Layout(width=\"150px\"))\n",
    "next_button.on_click(lambda btn: next_question())  # Bind function\n",
    "\n",
    "def play_sound(filename):\n",
    "    if os.path.exists(filename):  # Ensure file exists before playing\n",
    "        audio = Audio(filename, autoplay=True)\n",
    "        display(HTML(\"<style>.jp-OutputArea-output audio { display: none; }</style>\"))  # Hide the player\n",
    "        display(audio)  # Display the audio but assign it to `_` to suppress output\n",
    "\n",
    "def ask_question():\n",
    "    \"\"\"Displays a new question when 'Next Question' is clicked.\"\"\"\n",
    "    global current_question, score, lifeline_used\n",
    "    lifeline_used = {\"50:50\": False, \"Buddy\": False}  # Reset lifeline usage per question\n",
    "\n",
    "    if current_question >= len(questions):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"🎉 Game Over! Your final score: {score}/{len(questions)}\")\n",
    "        return\n",
    "\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Provide your answer.\")\n",
    "\n",
    "    q = questions[current_question]\n",
    "    progress_label.value = f\"🔢 Question {current_question + 1} / {len(questions)}\"\n",
    "\n",
    "    options = q['options'][:]\n",
    "    random.shuffle(options)\n",
    "\n",
    "    buttons = []\n",
    "\n",
    "    def check_answer(btn):\n",
    "        \"\"\"Checks the answer, disables interactions, and shows 'Next Question' button.\"\"\"\n",
    "        global current_question, score, lifeline_used\n",
    "\n",
    "        # Disable answer buttons\n",
    "        for button in buttons:\n",
    "            button.disabled = True\n",
    "\n",
    "        lifeline_used = {\"50:50\": True, \"Buddy\": True}\n",
    "\n",
    "        disable_lifelines()\n",
    "\n",
    "        # Show result\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            if btn.description == q['answer']:\n",
    "                score += 1\n",
    "                print(\"✅ Correct!\")\n",
    "                play_sound(\"data/correct.mp3\")  # Play sound for correct answer\n",
    "            else:\n",
    "                print(f\"❌ Incorrect. The correct answer was: {q['answer']}\")\n",
    "                play_sound(\"data/incorrect.mp3\")  # Play sound for correct answer\n",
    "\n",
    "        next_button.layout.display = \"block\"  # Show \"Next Question\" button\n",
    "\n",
    "    for option in options:\n",
    "        button = widgets.Button(description=option, layout=widgets.Layout(width='400px'))\n",
    "        button.on_click(check_answer)\n",
    "        buttons.append(button)\n",
    "\n",
    "    question_box.children = [widgets.HBox(\n",
    "        [progress_label, widgets.Label(layout=widgets.Layout(width=\"30px\")), next_button]),\n",
    "                                widgets.Label(q['question'])] + buttons\n",
    "\n",
    "    next_button.layout.display = \"none\"  # Hide the button initially\n",
    "\n",
    "    display_lifeline_buttons()\n",
    "    display(question_box)\n",
    "    display(output)\n",
    "\n",
    "\n",
    "def next_question():\n",
    "    \"\"\"Moves to the next question and clears the previous output.\"\"\"\n",
    "    global current_question\n",
    "    current_question += 1\n",
    "    with output:\n",
    "        clear_output(wait=True)  # Clear result BEFORE new question appears\n",
    "        print(\"Provide your answer.\")\n",
    "    ask_question()  # Load next question\n",
    "\n",
    "\n",
    "def display_lifeline_buttons():\n",
    "    \"\"\"Displays available lifelines as buttons and ensures they are only usable once per question.\"\"\"\n",
    "    lifeline_buttons = []\n",
    "    for lifeline in lifelines:\n",
    "        if lifelines[lifeline] > 0:\n",
    "            icon = \"🔹\" if lifeline == \"50:50\" else \"🧑‍🤝‍🧑\"\n",
    "            button = widgets.Button(description=f\"{icon} {lifeline} ({lifelines[lifeline]} left)\",\n",
    "                                    layout=widgets.Layout(width='150px'))\n",
    "            button.on_click(lambda btn, lf=lifeline: use_lifeline(lf))\n",
    "            lifeline_buttons.append(button)\n",
    "\n",
    "    lifeline_box.children = lifeline_buttons\n",
    "    display(lifeline_box)\n",
    "\n",
    "\n",
    "def use_lifeline(lifeline):\n",
    "    \"\"\"Handles lifeline usage and disables them after one use per question.\"\"\"\n",
    "    global current_question, lifeline_used\n",
    "\n",
    "    if lifelines[lifeline] > 0 and not lifeline_used[lifeline]:\n",
    "        lifelines[lifeline] -= 1\n",
    "        lifeline_used[lifeline] = True  # Prevent another lifeline from being used\n",
    "\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if lifeline == \"50:50\":\n",
    "                q = questions[current_question]\n",
    "                incorrect_options = [opt for opt in q['options'] if opt != q['answer']]\n",
    "                removed_options = random.sample(incorrect_options, 2)\n",
    "\n",
    "                new_buttons = []\n",
    "                for button in question_box.children[1:]:\n",
    "                    if button.description in removed_options:\n",
    "                        button.description = \"\"\n",
    "                        button.disabled = True\n",
    "                    new_buttons.append(button)\n",
    "\n",
    "                question_box.children = [widgets.HBox(\n",
    "                    [progress_label, widgets.Label(layout=widgets.Layout(width=\"30px\")), next_button])] + new_buttons\n",
    "\n",
    "            elif lifeline == \"Buddy\":\n",
    "                q = questions[current_question]\n",
    "            play_sound(\"data/joker.mp3\")  # Play sound for correct answer\n",
    "\n",
    "\n",
    "        display_lifeline_buttons()\n",
    "        disable_lifelines()\n",
    "\n",
    "    display(question_box)\n",
    "    display(output)\n",
    "\n",
    "\n",
    "def disable_lifelines():\n",
    "    global lifeline_used\n",
    "\n",
    "    \"\"\"Disables all lifeline buttons.\"\"\"\n",
    "    for button in lifeline_box.children:\n",
    "        if \"50:50\" in button.description and lifeline_used[\"50:50\"]:\n",
    "            button.disabled = True\n",
    "    \n",
    "        elif \"Buddy\" in button.description and lifeline_used[\"Buddy\"]:\n",
    "            button.disabled = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Wants To Be A Millionaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.snopes.com/uploads/images/radiotv/graphics/elephant1.jpg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the quiz\n",
    "ask_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences Between Classification and Regression\n",
    "\n",
    "- **Classification** predicts a **categorical** value:\n",
    "    - The output is selected from a **finite set of distinct categories** or classes (e.g., \"spam\" vs. \"not spam\", \"cat\" vs. \"dog\").\n",
    "    - The goal is to assign an instance to one of the predefined categories.\n",
    "\n",
    "- **Regression** predicts a **numerical** (continuous) value:\n",
    "    - The output is a **continuous variable**, which could take on an **infinite range of possible values** (e.g., predicting a person's age, house price, or temperature).\n",
    "    - Regression models can be used for:\n",
    "        - **Interpolation**: Making predictions within the range of the observed data.\n",
    "        - **Extrapolation**: Making predictions beyond the range of the observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression estimators in scikit-learn predict continuous target values, working similarly to classification estimators but using methods that minimize the error between predicted and actual values. Below are commonly used regression models:\n",
    "\n",
    "#### 1. **Linear Regression Models**\n",
    "\n",
    "**Linear regression** is a method that models the relationship between a dependent variable $y$ and one or more independent variables $X$ by fitting a linear equation to the observed data:\n",
    "\n",
    "$$\n",
    "y = \\underbrace{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p}_{\\hat{y}} + \\epsilon\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\beta_0$ is the intercept,\n",
    "- $\\beta_1, \\dots, \\beta_p$ are the coefficients for each feature,\n",
    "- $\\epsilon$ is the error term (residual).\n",
    "  \n",
    "- [**LinearRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html): Fits a simple linear model with coefficients to minimize the **Mean Squared Error (MSE)** between predicted and actual values.\n",
    "  \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- [**Ridge Regression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge): A linear regression model with **L2 regularization**, which penalizes large coefficients to prevent overfitting. The regularization term is added to the loss function:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{MSE} + \\alpha \\sum_{j=1}^{n} \\beta_j^2\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the regularization strength, controlling the penalty for large coefficients.\n",
    "\n",
    "- [**Lasso Regression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso): A linear regression model with **L1 regularization**, which encourages sparsity by driving some coefficients to zero. The regularization term is:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{MSE} + \\alpha \\sum_{j=1}^{n} |\\beta_j|\n",
    "$$\n",
    "\n",
    "Lasso is useful for feature selection, as it can eliminate less important features by setting their coefficients to zero.\n",
    "\n",
    "\n",
    "***Example: Derivation of the Maximum Likelihood Estimator with MSE***\n",
    "\n",
    "Consider a linear regression model in matrix form:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^n$ is the vector of observed values,\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{n \\times (p+1)}$ is the matrix of input features **with a column of ones** added to account for the intercept,\n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}$ is the vector of parameters, where the first element represents the intercept,\n",
    "- $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I})$ is the error term with variance $\\sigma^2$.\n",
    "\n",
    "Each observation $y_i$ follows a **normal distribution with mean $\\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ and variance $\\sigma^2$**. Therefore, the probability density function for each observation is:\n",
    "\n",
    "$$\n",
    "p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Assuming independence across all $n$ observations, the **joint likelihood function** for all observations is the product of individual likelihoods:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "This can be simplified as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2 \\right)\n",
    "$$\n",
    "\n",
    "In matrix notation, we can write this as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|^2 \\right)\n",
    "$$\n",
    "\n",
    "The **log-likelihood** is typically used for easier manipulation. Taking the logarithm of the likelihood function:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2) = \\log p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "This results in:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|^2\n",
    "$$\n",
    "\n",
    "To **maximize the likelihood**, we minimize the negative log-likelihood. Ignoring constants independent of $\\boldsymbol{\\beta}$, we can minimize the following:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\beta}) \\propto \\frac{1}{2} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|^2\n",
    "$$\n",
    "\n",
    "This is equivalent to minimizing the Mean Squared Error (MSE) objective:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\boldsymbol{\\beta}) = \\frac{1}{n} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|^2\n",
    "$$\n",
    "\n",
    "The solution for $\\boldsymbol{\\beta}$ that minimizes the MSE, including the intercept term, can be derived by solving the normal equations:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Thus, $\\boldsymbol{\\hat{\\beta}}$ is the **Maximum Likelihood Estimator (MLE)** for the parameters $\\boldsymbol{\\beta}$, including the intercept, under the assumption of normally distributed errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **K-Nearest Neighbor Regression (KNN Regression)**\n",
    "\n",
    "K-Nearest Neighbor regression is a non-parametric method that predicts the target value of a new data point by averaging the target values of its **k** nearest neighbors:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i\n",
    "$$\n",
    "\n",
    "Where $y_i$ are the target values of the nearest neighbors.\n",
    "\n",
    "- [**KNeighborsRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html): Predicts the output by finding the k-nearest points in the training data, based on distance (usually Euclidean), and averaging their target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Decision Tree Regression**\n",
    "\n",
    "Decision Tree regression splits the data into branches based on feature values to minimize the variance in the target values within each split. The tree is built by recursively partitioning the data:\n",
    "\n",
    "[**DecisionTreeRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html): This model works by partitioning the input space into regions and fitting a constant to each region. The tree is built by selecting splits that minimize metrics such as the **Mean Squared Error (MSE)** in each region.\n",
    "\n",
    "  🔹 **Difference to Gini**:  \n",
    "  - In **classification**, decision trees often use the **Gini impurity** or **entropy** to measure how \"pure\" a node is. Gini impurity measures how often a randomly chosen element would be incorrectly classified if it were labeled randomly based on the class distribution.\n",
    "  - In **regression**, instead of class labels, we predict continuous values. The tree chooses splits that minimize **MSE**, which measures how far the predicted values deviate from the actual values. Unlike Gini, which focuses on class separation, MSE in regression ensures that splits lead to regions with the least variance in predicted values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.saedsayad.com/images/Decision_tree_r1.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Neural Network Regression**\n",
    "\n",
    "Neural networks model complex relationships in the data by passing inputs through multiple layers of interconnected neurons. The network adjusts the weights of connections through backpropagation to minimize the error between predicted and actual values.\n",
    "\n",
    "- [**MLPRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html): This is a feedforward neural network that minimizes the error in predictions by adjusting weights using an optimization algorithm like stochastic gradient descent. It can capture complex, non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Each regression estimator has its strengths:\n",
    "- **Linear models** work well when the relationship between features and the target is linear.\n",
    "- **KNN** is a simple, non-parametric method effective in smaller datasets with non-linear relationships.\n",
    "- **Decision Trees** are powerful but prone to overfitting without pruning.\n",
    "- **Neural Networks** are highly flexible and capable of modeling complex, non-linear relationships but can require a large amount of data and computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "household_energy = pd.read_csv('data/household_energy.csv')\n",
    "household_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's have a look at some plots to determine how the features (we ignore the dates for now) are related to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a list of all columns that we are considering\n",
    "# features = [ 'age', 'temp', 'weight', 'length' ]\n",
    "features = [ 'House_Size_m2', 'Appliances', 'Temperature_C', 'Energy_Consumption_kWh']\n",
    "\n",
    "# create all combinations of considered columns\n",
    "combinations = itertools.combinations(features, 2)\n",
    "\n",
    "# create a figure and specify its size\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "# go through all combinations and create one plot for each\n",
    "figure_index = 1\n",
    "for combination in combinations:\n",
    "    # add a sub plot to the figure\n",
    "    axs = fig.add_subplot(2,3,figure_index)\n",
    "    \n",
    "    # plot the feature combination\n",
    "    # axs.scatter(fish[combination[0]], fish[combination[1]])\n",
    "    axs.scatter(household_energy[combination[0]], household_energy[combination[1]])\n",
    "\n",
    "    \n",
    "    # set the axis labels of the current sub plot\n",
    "    axs.set_xlabel(combination[0])\n",
    "    axs.set_ylabel(combination[1])\n",
    "        \n",
    "    # increase the figure index (otherwise all plots are drawn in the first subplot)\n",
    "    figure_index+=1\n",
    "\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It seems like there is a ***linear relation between house size and energy consumption***. We can fit a linear regression model and add it to the plot. First, we prepare the house size as a feature and the energy consumption as target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate features and target variable\n",
    "consumption = household_energy['Energy_Consumption_kWh']\n",
    "\n",
    "# special case: we only have one feature, so we must reshape the data here\n",
    "size = household_energy['House_Size_m2'].values.reshape(-1, 1)\n",
    "\n",
    "# create a train/test split\n",
    "size_train, size_test, size_target_train, size_target_test = train_test_split(\n",
    "    size, consumption, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's fit the linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# create and fit a linear regression\n",
    "consumption_estimator_size = LinearRegression()\n",
    "\n",
    "consumption_estimator_size.fit(size_train, size_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the temperature values and corresponding predictions for plotting\n",
    "sorted_indices = np.argsort(household_energy['House_Size_m2'].values)\n",
    "sorted_size = household_energy['House_Size_m2'].values[sorted_indices]\n",
    "sorted_features = size[sorted_indices]\n",
    "sorted_predictions = consumption_estimator_size.predict(sorted_features.reshape(-1, 1))\n",
    "\n",
    "# plot the training and test data points with different colors\n",
    "plt.scatter(size_train, size_target_train, c='green', label='train')\n",
    "plt.scatter(size_test, size_target_test, c='blue', label='test')\n",
    "\n",
    "# plot the predicted values\n",
    "plt.plot(household_energy['House_Size_m2'], consumption_estimator_size.predict(size), c='red', label='prediction')\n",
    "\n",
    "plt.xlabel('House Size m2')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"cosumption = {}*size + {}\".format(consumption_estimator_size.coef_[0], consumption_estimator_size.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now lets see if that also works for the **temperature** as feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special case: we only have one feature, so we must reshape the data here\n",
    "temperature = household_energy['Temperature_C'].values.reshape(-1, 1)\n",
    "\n",
    "# create a train/test split\n",
    "temperature_train, temperature_test, temperature_target_train, temperature_target_test = train_test_split(\n",
    "    temperature, consumption, test_size=0.4, random_state=42)\n",
    "\n",
    "# create and fit a linear regression\n",
    "consumption_estimator_temperature = LinearRegression()\n",
    "\n",
    "consumption_estimator_temperature.fit(temperature_train, temperature_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the temperature values and corresponding predictions for plotting\n",
    "sorted_indices = np.argsort(household_energy['Temperature_C'].values)\n",
    "sorted_temp = household_energy['Temperature_C'].values[sorted_indices]\n",
    "sorted_features = temperature[sorted_indices]\n",
    "sorted_predictions = consumption_estimator_temperature.predict(sorted_features.reshape(-1, 1))\n",
    "\n",
    "# plot the training and test data points with different colors\n",
    "plt.scatter(temperature_train, temperature_target_train, c='green', label='train')\n",
    "plt.scatter(temperature_test, temperature_target_test, c='blue', label='test')\n",
    "\n",
    "# plot the predicted values (now sorted by temperature)\n",
    "plt.plot(sorted_temp, sorted_predictions, c='red', label='prediction')\n",
    "\n",
    "plt.xlabel('Temperature_C')\n",
    "plt.ylabel('Energy_Consumption_kWh')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the model that was fitted (the regression formula)\n",
    "print(\"cosumption = {}*temperature + {}\".format(consumption_estimator_temperature.coef_[0], consumption_estimator_temperature.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted regression does not really match the data that we see. It seems that we need a polynomial regression here. We can fit such a regression by using a [```PolynomialFeatures``` transformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) that generates all possible feature combinations for the polynomial that we want to fit. On these transformed features, we can then use the linear regression again to fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transform the feature into a quadratic feature (2nd degree polynomial)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "temperature_poly = poly.fit_transform(temperature)\n",
    "\n",
    "# create a train/test split\n",
    "temperature_poly_train, temperature_poly_test, temperature_poly_target_train, temperature_poly_target_test = train_test_split(\n",
    "    temperature_poly, consumption, test_size=0.4, random_state=42)\n",
    "\n",
    "# create and fit a linear regression on the polynomial features\n",
    "consumption_estimator_temperature_poly = LinearRegression()\n",
    "consumption_estimator_temperature_poly.fit(temperature_poly_train, temperature_poly_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the temperature values and corresponding predictions for plotting\n",
    "sorted_indices = np.argsort(household_energy['Temperature_C'].values)\n",
    "sorted_temp = household_energy['Temperature_C'].values[sorted_indices]\n",
    "sorted_features_poly = temperature_poly[sorted_indices]\n",
    "sorted_predictions_poly = consumption_estimator_temperature_poly.predict(sorted_features_poly)\n",
    "sorted_predictions = consumption_estimator_temperature.predict(sorted_features_poly[:, 1].reshape(-1, 1))\n",
    "\n",
    "# plot the training and test data points with different colors\n",
    "plt.scatter(temperature_poly_train[:, 1], temperature_poly_target_train, c='green', label='train')\n",
    "plt.scatter(temperature_poly_test[:, 1], temperature_poly_target_test, c='blue', label='test')\n",
    "\n",
    "# plot the predicted values (now sorted by temperature)\n",
    "plt.plot(sorted_temp, sorted_predictions_poly, c='orange', label='quadratic prediction')\n",
    "# plot the predicted values (now sorted by temperature)\n",
    "plt.plot(sorted_temp, sorted_predictions, c='red', label='non-quadratic prediction')\n",
    "\n",
    "plt.xlabel('Temperature_C')\n",
    "plt.ylabel('Energy_Consumption_kWh')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the quadratic regression formula\n",
    "coef = consumption_estimator_temperature_poly.coef_\n",
    "intercept = consumption_estimator_temperature_poly.intercept_\n",
    "print(f\"consumption = {coef[2]:.4f}*temperature^2 + {coef[1]:.4f}*temperature + {intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://pbs.twimg.com/media/CUogtjOUsAAovyR.jpg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Generate fixed noisy quadratic dataset\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(-10, 10, 15)  # X values\n",
    "X_scatter = np.linspace(-10, 10, 500)  # X values\n",
    "y_data = X_data**2 + np.random.normal(scale=10, size=X_data.shape)  # Quadratic with noise\n",
    "\n",
    "# Reshape X_data for sklearn\n",
    "X_data = X_data.reshape(-1, 1)\n",
    "\n",
    "# Interactive function\n",
    "def plot_polynomial_regression(model_degree=2, show_green_line=False):\n",
    "    \"\"\"Fits a polynomial regression model to the fixed dataset and updates the plot.\"\"\"\n",
    "    \n",
    "    # Create a new figure each time to prevent overlapping plots\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Fit polynomial regression model\n",
    "    model = make_pipeline(PolynomialFeatures(model_degree), LinearRegression())\n",
    "    model.fit(X_data, y_data)\n",
    "\n",
    "    # Predict on sorted values\n",
    "    sorted_indices = np.argsort(X_data[:, 0])\n",
    "    sorted_X = X_data[sorted_indices]\n",
    "    sorted_predictions = model.predict(sorted_X)\n",
    "\n",
    "    sorted_predictions_scatter = model.predict(X_scatter.reshape(-1, 1))\n",
    "\n",
    "    # Scatter plot of noisy data\n",
    "    ax.scatter(X_data, y_data, c='blue', label='Noisy Data')\n",
    "\n",
    "    # Plot fitted polynomial regression curve\n",
    "    ax.plot(sorted_X, sorted_predictions, c='red', label=f'Polynomial Regression (Degree {model_degree})')\n",
    "\n",
    "    # Plot the true quadratic function (thicker line for better visibility)\n",
    "    if show_green_line:\n",
    "        ax.plot(X_scatter, sorted_predictions_scatter, c='green', linestyle='dashed', label='True Function (y = x²)', linewidth=2)\n",
    "\n",
    "    # Print regression equation\n",
    "    model_coefs = model.named_steps['linearregression'].coef_\n",
    "    intercept = model.named_steps['linearregression'].intercept_\n",
    "    print(f\"Regression Model: y = {' + '.join([f'{coef:.2f}*x^{i}' for i, coef in enumerate(model_coefs)])} + {intercept:.2f}\")\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Polynomial Regression with Degree {model_degree}\")\n",
    "    \n",
    "    # Display the updated plot\n",
    "    plt.show()\n",
    "\n",
    "# Create an input widget to manually type in the polynomial degree (between 1 and 50)\n",
    "degree_input = widgets.IntText(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    description=\"Model Degree:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Function to ensure the degree stays within valid bounds\n",
    "def on_degree_change(change):\n",
    "    if change['new'] < 1:\n",
    "        degree_input.value = 1\n",
    "    elif change['new'] > 100:\n",
    "        degree_input.value = 100\n",
    "\n",
    "# Attach the observer to the degree_input widget\n",
    "degree_input.observe(on_degree_change, names='value')\n",
    "\n",
    "# Create button to toggle the green line\n",
    "button = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description=\"Show True Poly\",\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip=\"Click to toggle the green line\"\n",
    ")\n",
    "\n",
    "# Ensure updates when input or button changes\n",
    "interactive_plot = widgets.interactive(plot_polynomial_regression, model_degree=degree_input, show_green_line=button)\n",
    "\n",
    "# Display input field, button, and plot\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a continuous target variable, it does not make sense to count how often we predicted the exact correct value. Instead, we evaluate how close our prediction is to the correct value using common regression metrics. In this exercise, we will talk about the following measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Mean Squared Error (MSE)**\n",
    "\n",
    "Mean Squared Error (MSE) measures the average squared difference between the predicted values $\\hat{y}$ and the actual target values $y$. It penalizes larger errors more than smaller ones, making it sensitive to outliers:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points,\n",
    "- $y_i$ is the actual value for the $i$-th data point,\n",
    "- $\\hat{y}_i$ is the predicted value for the $i$-th data point.\n",
    "\n",
    "### 2. **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "Root Mean Squared Error (RMSE) is the square root of MSE. It gives the error in the same units as the target variable and is easier to interpret. Like MSE, RMSE is also sensitive to larger errors.\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2} = = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "### 3. **Coefficient of Determination (R²)**\n",
    "\n",
    "The **coefficient of determination** ($R^2$) measures the proportion of variance in the target variable that is explained by the regression model. It provides insight into the goodness of fit:\n",
    "\n",
    "- $R^2 = 1$ indicates a perfect fit, meaning the model explains all the variance.\n",
    "- $R^2 = 0$ means the model performs no better than simply predicting the mean of the target variable.\n",
    "- Negative values of $R^2$ suggest that the model is worse than a naive mean prediction.\n",
    "\n",
    "The formula for $R^2$ is:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}{\\sum_{i=1}^{n} \\left( y_i - \\bar{y} \\right)^2}\n",
    "$$\n",
    "\n",
    "A higher $R^2$ value indicates a better fit, but it does not imply causation or suitability for all types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "- **MSE (Mean Squared Error)** and **RMSE (Root Mean Squared Error)** penalize larger errors due to squaring the residuals. They are useful for understanding the absolute prediction accuracy in the same units as the target variable.  \n",
    "  - **MSE** is more sensitive to large errors (outliers) since errors are squared, making it a better choice when large deviations should be heavily penalized.  \n",
    "  - **RMSE** provides a more interpretable metric in the same unit as the target variable, making it easier to understand the typical prediction error.\n",
    "\n",
    "- **$R^2$ (Coefficient of Determination)** measures how well the model explains the variance in the target variable. Unlike MSE/RMSE, it is a **relative measure of fit** rather than an absolute error metric.  \n",
    "  - A higher **$R^2$** means the model explains more of the variability in the data, while a low or negative **$R^2$** suggests poor explanatory power.  \n",
    "  - However, **$R^2$ does not indicate whether predictions are accurate in absolute terms**, making it less useful when exact error magnitudes are important.\n",
    "\n",
    "🔹 **Key Difference**:  \n",
    "- **MSE/RMSE** are absolute error metrics useful when you care about the actual magnitude of errors.  \n",
    "- **$R^2$** is a relative goodness-of-fit measure, helping to understand how well the model captures variance but without direct insight into actual prediction errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "# predict the values for the test data\n",
    "predictions = consumption_estimator_temperature.predict(temperature_test)\n",
    "\n",
    "# evaluate using different measures\n",
    "mse = mean_squared_error(temperature_target_test, predictions)\n",
    "r2 = r2_score(temperature_target_test, predictions)\n",
    "\n",
    "print(\"MSE (without poly):\", mse)\n",
    "print(\"RMSE (without poly):\", sqrt(mse))\n",
    "print(\"R^2 (without poly):\", r2, \"\\n\")\n",
    "\n",
    "# predict the values for the test data\n",
    "predictions_poly = consumption_estimator_temperature_poly.predict(temperature_poly_test)\n",
    "\n",
    "# evaluate using different measures\n",
    "mse = mean_squared_error(temperature_poly_target_test, predictions_poly)\n",
    "r2 = r2_score(temperature_poly_target_test, predictions_poly)\n",
    "\n",
    "print(\"MSE (with poly):\", mse)\n",
    "print(\"RMSE (with poly):\", sqrt(mse))\n",
    "print(\"R^2 (with poly):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of identifying and selecting the most relevant features (input variables) for building a predictive model. This helps improve model performance by reducing overfitting, improving accuracy, and reducing computation time. The following methods are common approaches to feature selection:\n",
    "\n",
    "- [```f_regression``` function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "    - Performs an [F-Test](https://en.wikipedia.org/wiki/F-test) to determine the relationship between each feature and the target variable in a regression context.\n",
    "    - The F-Test compares the variance explained by the feature to the unexplained variance (error):\n",
    "    \n",
    "    $$F = \\frac{\\text{explained variance by feature}}{\\text{unexplained variance (error)}}$$\n",
    "    \n",
    "    - Features with higher F-values are considered more important for predicting the target.\n",
    "\n",
    "\n",
    "- [```SelectKBest``` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "    - Selects the top ```k``` features based on a scoring function, such as ```f_regression``` or others like mutual information.\n",
    "    - This method allows you to specify the number of features to retain, helping focus the model on the most important variables.\n",
    "\n",
    "\n",
    "- [```SelectFwe``` class](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html)\n",
    "    - This method selects all features with a p-value below a specified ```threshold``` $\\tau$, based on the results of the statistical test from a scoring function (e.g., ```f_regression```).\n",
    "    - The p-value helps assess whether the feature's effect on the target is statistically significant. The smaller the p-value, the more significant the feature.\n",
    "    - The goal is to retain features where:\n",
    "\n",
    "    $$p\\text{-value} < \\tau$$\n",
    "    \n",
    "    - This method ensures that only features with significant relationships to the target are kept.\n",
    "\n",
    "\n",
    "- **Recursive Feature Elimination (RFE)**: [```RFECV``` class](https://scikit-learn.org/stable/modules/feature_selection.RFECV.html)\n",
    "    - RFE is an iterative method that fits a model (any estimator that provides feature importance, such as a decision tree or linear regression) and removes the least important features at each step.\n",
    "    - This method uses cross-validation to determine the optimal number of features by evaluating performance at each iteration and selecting the number of features that results in the best cross-validation score.\n",
    "    - The steps in RFECV are:\n",
    "      1. Fit the model.\n",
    "      2. Rank features by importance.\n",
    "      3. Remove the least important features.\n",
    "      4. Repeat until the optimal number of features is selected based on cross-validation.\n",
    "\n",
    "For more details, have a look at the [feature selection documentation of scikit-learn](https://scikit-learn.org/stable/modules/feature_selection.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# separate features and target variable\n",
    "household_energy_target = household_energy['Energy_Consumption_kWh']\n",
    "household_energy_lean = household_energy.drop(columns=['Date', 'Energy_Consumption_kWh'])\n",
    "\n",
    "# create a train/test split\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    household_energy_lean, household_energy_target,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try an F Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# Step 1: Create a transformer\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# Step 2: Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "\n",
    "# Step 3: Apply the polynomial transformation on the scaled data\n",
    "data_train_poly = transformer.fit_transform(data_train_scaled)\n",
    "\n",
    "# Step 4: Run the F-Test\n",
    "f, pval = f_regression(data_train_poly, target_train)\n",
    "\n",
    "# Step 5: Prepare a dataframe to inspect the results\n",
    "stat = pd.DataFrame({ \n",
    "    'feature': transformer.get_feature_names_out(household_energy_lean.columns), \n",
    "    'F value': f, \n",
    "    'p value': pval \n",
    "})\n",
    "\n",
    "# Step 6: Format the p-values and F-values for readability\n",
    "stat['p value'] = round(stat['p value'], 2)\n",
    "stat['F value'] = round(stat['F value'], 2)\n",
    "\n",
    "# Step 7: Show the results\n",
    "display(stat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also make feature selection a part of the sklearn pipeline and have it automatically train a model with the best found features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the estimator and feature selection method\n",
    "estimator = LinearRegression()\n",
    "best = SelectFwe(f_regression, alpha=0.05)\n",
    "transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "scaler = StandardScaler()  # Assuming scaler was defined earlier\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([ ('scaler', scaler), \n",
    "                      ('transformer', transformer), \n",
    "                      ('feature_selection', best), \n",
    "                      ('estimator', estimator)])\n",
    "\n",
    "# Fit the regression model on the training data\n",
    "pipeline.fit(data_train, target_train)\n",
    "\n",
    "# Predict the values for the test data\n",
    "predictions = pipeline.predict(data_test)\n",
    "\n",
    "# Evaluate using different measures\n",
    "mse = mean_squared_error(target_test, predictions)\n",
    "r2 = r2_score(target_test, predictions)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", np.sqrt(mse))\n",
    "print(\"R^2:\", r2)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = pipeline.named_steps['feature_selection'].get_support()\n",
    "feature_index = 0\n",
    "\n",
    "# Print the model that was fitted (the regression formula)\n",
    "for i, f in enumerate(pipeline.named_steps['transformer'].get_feature_names_out(household_energy_lean.columns)):\n",
    "    if selected_features[i]:\n",
    "        if i > 0:\n",
    "            print(\" + \", end='')\n",
    "        print(\"{}*{}\".format(pipeline.named_steps['estimator'].coef_[feature_index], f), end='')\n",
    "        feature_index += 1\n",
    "print(\" + {}\".format(pipeline.named_steps['estimator'].intercept_))\n",
    "\n",
    "# Sort by a specific feature (e.g., the first feature, assuming it's meaningful for ordering)\n",
    "sorted_indices = np.argsort(data_test.iloc[:, 0])  # Sort by the first feature (column 0)\n",
    "\n",
    "# Sort predictions and corresponding target values based on the sorted indices\n",
    "sorted_predictions = predictions[sorted_indices]\n",
    "sorted_target_test = target_test.iloc[sorted_indices]\n",
    "\n",
    "# Plot predictions vs ground truth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_target_test.values, label='Ground Truth', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(sorted_predictions, label='Predictions', marker='x', linestyle='--', color='red')\n",
    "plt.plot(sorted_predictions - sorted_target_test.values, label='Diff', marker='x', linestyle='--', color='black')\n",
    "\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.title('Predictions vs Ground Truth')\n",
    "plt.xlabel('Random Index')\n",
    "plt.ylabel('Energy Consumption (kWh)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen & Paper Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### P&P (1) Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we aim to minimize the **sum of squared errors (SSE)** between the actual values $y$ and the predicted values $\\hat{y}$, where:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "##### The formulas for the parameters in the regression model are:\n",
    "\n",
    "1. **Slope** ($\\beta_1$):\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "2. **Intercept** ($\\beta_0$):\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\bar{x}$ is the mean of $x$ values.\n",
    "- $\\bar{y}$ is the mean of $y$ values.\n",
    "\n",
    "Given the following dataset:\n",
    "\n",
    "| $x$ | $y$  |\n",
    "|-----|------|\n",
    "| 1   | 2    |\n",
    "| 2   | 4    |\n",
    "| 3   | 5    |\n",
    "| 4   | 4    |\n",
    "| 5   | 5    |\n",
    "\n",
    "Fit a simple linear regression model $y = \\beta_0 + \\beta_1 x$ using the **least squares method**.\n",
    "1. Calculate the coefficients $\\beta_0$ and $\\beta_1$.\n",
    "2. Provide the equation of the best-fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P&P (2) Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps to Solve:\n",
    "\n",
    "1. **Calculate the means of $x$ and $y$:**\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1 + 2 + 3 + 4 + 5}{5} = 3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{2 + 4 + 5 + 4 + 5}{5} = 4\n",
    "$$\n",
    "\n",
    "2. **Calculate the slope $\\beta_1$:**\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{(1 - 3)(2 - 4) + (2 - 3)(4 - 4) + (3 - 3)(5 - 4) + (4 - 3)(4 - 4) + (5 - 3)(5 - 4)}{(1 - 3)^2 + (2 - 3)^2 + (3 - 3)^2 + (4 - 3)^2 + (5 - 3)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{4 + 0 + 0 + 0 + 2}{4 + 1 + 0 + 1 + 4} = \\frac{6}{10} = 0.6\n",
    "$$\n",
    "\n",
    "3. **Calculate the intercept $\\beta_0$:**\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} = 4 - (0.6 \\cdot 3) = 4 - 1.8 = 2.2\n",
    "$$\n",
    "\n",
    "4. **Equation of the best-fit line:**\n",
    "\n",
    "$$\n",
    "y = 2.2 + 0.6x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Explanation of Formula Derivation:\n",
    "\n",
    "The formula for $\\beta_1$ (slope) comes from the idea of minimizing the sum of squared errors (SSE) in the model. We take the partial derivative of SSE with respect to $\\beta_0$ and $\\beta_1$, set them equal to 0, and solve for the optimal values of the parameters.\n",
    "\n",
    "The intercept $\\beta_0$ is calculated after finding $\\beta_1$, ensuring that the model passes through the mean of the data $(\\bar{x}, \\bar{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P&P (2) Understanding the Bias-Variance Trade-off\n",
    "\n",
    "Consider two models for predicting the target variable $y$:\n",
    "\n",
    "**Model 1**: A linear regression model that predicts $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$.\n",
    "\n",
    "**Model 2**: A more complex model with higher-degree polynomial terms and interactions: $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2$.\n",
    "\n",
    "1. Which model is likely to have higher **bias**?\n",
    "2. Which model is likely to have higher **variance**?\n",
    "3. Explain how each model fits into the **bias-variance trade-off**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P&P (2) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model 1 (Linear Model)** is likely to have **higher bias** because it assumes a linear relationship between $x_1$, $x_2$, and $y$, which may not capture complex patterns in the data.\n",
    "   \n",
    "2. **Model 2 (Complex Model)** is likely to have **higher variance** because it is more flexible and can fit more complex relationships, potentially overfitting the training data. This flexibility means the model is more sensitive to fluctuations in the data.\n",
    "\n",
    "3. **Bias-Variance Trade-off**:\n",
    "\n",
    "   - **Model 1** has high bias but low variance. It may underfit the data by oversimplifying the relationship between the features and the target, resulting in predictions that are consistently off from the true values (high bias).\n",
    "   \n",
    "   - **Model 2** has low bias but high variance. It is more likely to fit the training data very well but perform poorly on unseen data due to overfitting, making its predictions vary significantly with different training sets (high variance).\n",
    "\n",
    "The **bias-variance trade-off** describes the balance between underfitting (high bias, low variance) and overfitting (low bias, high variance). The goal is to find a model that minimizes both, typically through cross-validation and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.kdnuggets.com/wp-content/uploads/arya_biasvariance_tradeoff_4.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P&P (3) Regression Exam\n",
    "\n",
    "You, Tim, and Lara analyze an agriculture dataset for your team project.\n",
    "\n",
    "The dataset includes:  \n",
    "- **X₁**: Avg. monthly temperature (°C)  \n",
    "- **X₂**: Monthly rainfall (mm)  \n",
    "- **X₃**: Sunny days per month  \n",
    "- **y**: Crop yield (tons)  \n",
    "\n",
    "The goal is to build a regression model to predict **y** using **X₁, X₂, and X₃**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P&P (3.1) Regressor Evaluation Metrics\n",
    "\n",
    "Tim and Lara each trained regression models. You chose the **R²** score as the primary metric.  \n",
    "Tim's model achieved an **R² score of 0.55**, while Lara's model achieved an **R² score of 0.86**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P&P (3.1 a) \n",
    "\n",
    "Why is **R²** commonly used, and what are its key characteristics? Based on this, which model would you choose: Tim's or Lara's? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **R²** score measures the proportion of variance in the target variable explained by the model.  \n",
    "- A higher **R²** indicates better performance as the model captures more variability.  \n",
    "- **Choice:** Lara's model (**R² = 0.86**) is preferred since it explains more variance than Tim's model (**R² = 0.55**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### P&P (3.1 b) \n",
    "\n",
    "Below are two plots comparing the models' predictions to real values. Assign the **R²** scores to the plots by completing the titles (e.g., Tim's Regressor).\n",
    "\n",
    "![Comparison Plots](imgs/comparison_plots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left: Tim's Regressor (**R² = 0.55**), Right: Lara's Regressor (**R² = 0.86**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P&P (3.1 c) \n",
    "\n",
    "In addition to **R²**, suggest another evaluation metric that could provide further insights into the model's performance. Briefly explain its relevance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE):**  \n",
    "- Measures the average absolute difference between predictions and actual values.  \n",
    "- MAE is robust to outliers and provides an interpretable error in the original units (tons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P&P (3.2) Regressor Evaluation Metrics\n",
    "\n",
    "Tim trained his model using an ANN, while Lara used a Regression Tree model. To complement these approaches, you decide to test a **Linear Regression model**:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### P&P (3.2 a) \n",
    "\n",
    "Explain the components of this formula and briefly describe how the model can be trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components:**\n",
    "- **$β₀$**: Intercept, the baseline prediction when **$X₁, X₂, X₃ = 0$**.\n",
    "- **$β₁, β₂, β₃$**: Coefficients representing the effect of **$X₁, X₂, X₃$** on **$y$**.\n",
    "- **$\\epsilon$**: Error term capturing unexplained variability.\n",
    "\n",
    "**Training Methods:**\n",
    "- **Least Squares:** Minimizes the sum of squared errors between predictions and actual values.\n",
    "- **Maximum Likelihood Estimation (MLE):** Finds coefficients by maximizing the likelihood of observing the given data under a Gaussian assumption for errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P&P (3.2 b) \n",
    "\n",
    "Ridge and Lasso are two regularization techniques often used with regression models. Briefly describe the intuition behind these techniques and how they differ from each other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Techniques:**\n",
    "- **Ridge Regression:** Adds $L_2$-penalty $ \\lambda \\sum_{j=1}^n \\beta_j^2 $ to shrink coefficients, reducing overfitting.\n",
    "- **Lasso Regression:** Adds $L_1$-penalty $ \\lambda \\sum_{j=1}^n |\\beta_j| $ to shrink and potentially set coefficients to zero, enabling feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P&P (3.2 c) \n",
    "\n",
    "\n",
    "Describe why the initial linear regression model might have been insufficient in capturing the relationship between crop yield and monthly rainfall. Propose how you would modify the linear regression model.  \n",
    "\n",
    "![Rainfall Plot](imgs/rainfall.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model might fail since the relationship between $X_2$ (rainfall) and $y$ (crop yield) is quadratic, i.e.,  non-linear.  \n",
    "**Solution:** \n",
    "\n",
    "Modify the model to include polynomial features, such as $X_2^2$, to better capture the relationship:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_2^2 + \\epsilon.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
