{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b29855",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc83f16",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/03_Classification'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772418b",
   "metadata": {},
   "source": [
    "# Some Classifiers, Overfitting, and Evaluation Metrics\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/classification_meme.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "In this exercise, for the Python examples, we will use a dataset about **GOLF** which you can find in **data/golf.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load the data\n",
    "golf = pd.read_csv('data/golf.csv')\n",
    "\n",
    "golf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the data\n",
    "golf_preprocessed = golf.copy()\n",
    "\n",
    "# create a variable with the values of the target variable\n",
    "golf_target = golf['Play']\n",
    "\n",
    "# and remove it from the dataframe so it only contains the features that our model should use\n",
    "golf_preprocessed = golf.drop(columns='Play')\n",
    "\n",
    "# encode the target variable into a numeric value\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "golf_target = label_encoder.fit_transform(golf_target)\n",
    "\n",
    "# encode the Outlook and Wind features\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoded = pd.DataFrame(\n",
    "    encoder.fit_transform(golf_preprocessed[['Outlook', 'Wind']]).toarray(), \n",
    "    columns=encoder.get_feature_names_out(['Outlook', 'Wind'])\n",
    ")\n",
    "\n",
    "golf_preprocessed = golf_preprocessed.drop(columns=['Outlook', 'Wind'])\n",
    "golf_preprocessed = golf_preprocessed.join(encoded)\n",
    "golf_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807537e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To properly evaluate our model's performance on unseen data, we need a **separate test set**. We can either use a **train-test split** or a completely separate test set, which you can find in **data/golf_testset.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd92fd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "golf_test = pd.read_csv('data/golf_testset.csv')\n",
    "\n",
    "# create a variable with the values of the target variable\n",
    "golf_target_test = golf_test['Play']\n",
    "\n",
    "# and remove it from the dataframe so it only contains the features that our model should use\n",
    "golf_test = golf_test.drop(columns='Play')\n",
    "\n",
    "# encode the target variable into a numeric value\n",
    "golf_target_test = label_encoder.transform(golf_target_test)\n",
    "\n",
    "encoded = pd.DataFrame(\n",
    "    encoder.transform(golf_test[['Outlook', 'Wind']]).toarray(), \n",
    "    columns=encoder.get_feature_names_out(['Outlook', 'Wind'])\n",
    ")\n",
    "golf_test = golf_test.drop(columns=['Outlook', 'Wind'])\n",
    "\n",
    "golf_test = golf_test.join(encoded)\n",
    "golf_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed932a",
   "metadata": {},
   "source": [
    "# Nearest Centroid Classifier (NCC): Classification as Simple as It Gets\n",
    "\n",
    "a.k.a. _Rocchio classifier_\n",
    "\n",
    "## What is Nearest Centroid?\n",
    "Nearest Centroid is a simple and intuitive machine learning algorithm used for classification. Given a data point, it simply assigns it the label (class) of the training sample whose mean or centroid is closest to it. \n",
    "\n",
    "## How Does Nearest Centroid Work?\n",
    "1. Compute the centroid for each class (i.e., the point that lies in the middle of all data points in a class).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/nearest_centroid_learning.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "2. Calculate the distance (e.g., Euclidean distance) from a new data point to all centroids.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/nearest_centroid_predict.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "3. Assign the class label of the nearest centroid to the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed46b2",
   "metadata": {},
   "source": [
    "## Nearest Centroid for Golf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870551b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Nearest Centroid classifier\n",
    "from sklearn.neighbors import NearestCentroid \n",
    "ncc = NearestCentroid()\n",
    "ncc.fit(golf_preprocessed, golf_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the model (predict)\n",
    "golf_test_prediction = ncc.predict(golf_test)\n",
    "\n",
    "# add the prediction and gold standard (target) to the dataframe\n",
    "golf_comparison = golf_test.assign(Play=golf_target_test)\n",
    "golf_comparison = golf_comparison.assign(predictionPlay=golf_test_prediction)\n",
    "\n",
    "golf_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e13b61",
   "metadata": {},
   "source": [
    "## $k$-NN vs. Nearest Centroid: Isn't it all the same?\n",
    "\n",
    "- **Standard problem** (e.g., separable ellipsses): both approaches work reasonably well\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/knn_vs_ncc_standard.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- **Label noise** (e.g., some data points are wrongly labelled): $k$-NN losses performance, but NCC stays stable\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/knn_vs_ncc_label_noise.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- **Unbalanced data**: $k$-NN may lose performance (towards the class boundary), but NCC stays strong\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/knn_vs_ncc_imbalance.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- **Outliers**: $k$-NN is unaffected, NCC loses performance (i.e., outliers can move centroids significantly) \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/knn_vs_ncc_outliers.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "- **Multiple clusters**: $k$-NN is unaffected, NCC fails\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/knn_vs_ncc_multiple_clusters.png\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "| Problem | k-NN | Nearest Centroid |\n",
    "|---|---|---|\n",
    "| Robustness | robust to outliers   | robust to label noise & class imbalance |\n",
    "| Classification time | slow (linear in number of data points) | fast (linear in number of classes) |\n",
    "| Memory requirements | High (store all data points) | Low (story only the centroids) |\n",
    "\n",
    "Which classifier is better **depends on the problem at hand**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803340d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "## What is a Decision Tree?\n",
    "A Decision Tree (DT) is a supervised machine learning algorithms used for classification and regression. It predicts the value of the target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "\n",
    "## How does a Deicison Tree Classifier work?\n",
    "\n",
    "\n",
    "1. Start at the **root node**, which represents the entire dataset.\n",
    "2. Choose the attribute that best splits the data into subsets.\n",
    "    - We determine the best split by measuring the node impurity of the split (i.e., how well it divides the data). We can use measures like Gini Index, Information Gain etc \n",
    "    \n",
    "3. Use the chosen attribute to create branches that split the dataset into different groups based on the feature values.\n",
    "    - Specifying the attribute test condition depends on the _attribute types_ and _number of ways to split_\n",
    "    - **Important**: test all possible splits!\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <img src=\"imgs/decision_tree1.png\" style=\"width: 40%;\">\n",
    "    <img src=\"imgs/decision_tree2.png\" style=\"width: 50%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "4. Repeat for Each Branch\n",
    "    - For each branch (subset of the data), the process is repeated: the algorithm chooses the best feature to split on, based on how well it separates the data in that subset.\n",
    "   - Continue this recursive splitting until certain stopping conditions are met \n",
    "       - Maximum Depth: The tree reaches a predefined maximum depth.\n",
    "       - Pure Nodes: All samples in a node belong to the same class (i.e., the node is \"pure\").\n",
    "       - Insufficient Data: There are not enough data points to further split. \n",
    "       - Low Information Gain: Further splits do not lead to significant improvement in the decision-making process.\n",
    "\n",
    "5. Assign a Class Label to Each Leaf Node\n",
    "    - Once the tree is built, each **leaf node** (a terminal node with no further splits) is assigned a class label based on the majority class of the data points in that leaf.\n",
    "\n",
    "6. Make Predictions\n",
    "    - For a new input data point, the decision tree traverses the tree starting from the root, moving along the branches based on the values of the features of the input data. It follows the splits until it reaches a leaf node, and the class label of that leaf node is the predicted class for the input data.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/decision_tree3.png\" style=\"width: 75%;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd384a",
   "metadata": {},
   "source": [
    "## How to Choose Attribute Test Conditions?\n",
    "\n",
    "### Based on the _Number of Ways to Split_\n",
    "- **Binary split**  \n",
    "- **Multi-way split**  \n",
    "\n",
    "### Based on the _Attribute Types_\n",
    "\n",
    "1. **Nominal Values**  \n",
    "    - **Binary split**: Divide values into 2 subsets.\n",
    "\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"imgs/nominal_feat_binary_split.png\" style=\"width: 75%;\">\n",
    "        </div>\n",
    "\n",
    "    - **Multi-way split**: Use as many partitions as distinct values.\n",
    "\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"imgs/nominal_feat_multiway_split.png\" style=\"width: 40%;\">\n",
    "        </div>\n",
    "\n",
    "2. **Ordinal Values**  \n",
    "    - **Binary split**: Divide values into 2 subsets **while keeping the order**.\n",
    "\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"imgs/ordinal_feat_binary_split.png\" style=\"width: 75%;\">\n",
    "        </div>\n",
    "\n",
    "    - **Multi-way split**: Use as many partitions as distinct values.\n",
    "\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"imgs/ordinal_feat_multiway_split.png\" style=\"width: 40%;\">\n",
    "        </div>\n",
    "\n",
    "3. **Continuous Values**  \n",
    "    - **Binary split**: Discretize to form an ordinal attribute (e.g., using binning).\n",
    "\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"imgs/cont_feat_binary_split.png\" style=\"width: 30%;\">\n",
    "        </div>\n",
    "\n",
    "    - **Multi-way split**: (A < v) or (A ≥ v).\n",
    "\n",
    "        <div style=\"text-align: center;\">\n",
    "            <img src=\"imgs/cont_feat_multiway_split.png\" style=\"width: 30%;\">\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0233833",
   "metadata": {},
   "source": [
    "## How to Determine the Best Split?\n",
    "\n",
    "We use a measure of **node impurity**: in this lecture, we use the **GINI Index**.\n",
    "\n",
    "### GINI Index\n",
    "\n",
    "The GINI Index measures how often a randomly chosen element would be incorrectly classified.\n",
    "\n",
    "$ GINI (t) = 1 - \\sum_j [p(j|t)]^2 $\n",
    "\n",
    "where $ p(j|t) $ is the relative frequency of class $ j $ at node $ t $.\n",
    "\n",
    "- **Minimum (0):** All records belong to one class → most informative.\n",
    "- **Maximum** $ (1 - \\frac{1}{n_c}) $: All records are equally distributed among all classes → least informative.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/gini_index.png\" style=\"width: 50%;\">\n",
    "</div>\n",
    "\n",
    "### Splitting Based on GINI\n",
    "\n",
    "For a node $ p $ split into $ k $ partitions (children), we compute the quality of the split as:\n",
    "\n",
    "$ GINI_{split} = \\sum_{i=1}^k \\frac{n_i}{n} GINI(i) $\n",
    "\n",
    "where $ n_i $ = number of records at child node $ i $ and $ n $ = number of records at node $ p $.\n",
    "\n",
    "**Intuition:** Weight the GINI index of each partition according to the size of the partition.\n",
    "\n",
    "### Computing the GINI Split\n",
    "\n",
    "Split into two partitions and compute the **purity gain** to decide the best split (i.e., highest purity gain = lowest $ GINI_{split} $).\n",
    "\n",
    "$ \\text{Purity Gain} = \\text{Impurity before splitting} - \\text{Impurity after splitting} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb54558",
   "metadata": {},
   "source": [
    "## Let's build a decision tree (step-by-step)\n",
    "\n",
    "A hospital would like to support the doctors with their diagnoses. For this purpose, health data from\n",
    "various patients were collected (see table below). Build a decision tree using the GINI index in order\n",
    "to be able to automatically classify a patient’s state of health. List all calculations in your solution and\n",
    "draw the resulting decision tree!\n",
    "\n",
    "\n",
    "| # | Heart Rate | Blood Pressure | Class   |\n",
    "| - | ---------- | -------------- | ------- |\n",
    "| 1 | irregular  | normal         | ill     |\n",
    "| 2 | regular    | normal         | healthy |\n",
    "| 3 | irregular  | abnormal       | ill     |\n",
    "| 4 | irregular  | abnormal       | ill     |\n",
    "| 5 | regular    | abnormal       | healthy |\n",
    "| 6 | regular    | abnormal       | ill     |\n",
    "| 7 | regular    | normal         | healthy |\n",
    "| 8 | regular    | normal         | healthy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442f5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree for the Golf Dataset\n",
    "\n",
    "The decision tree classification is implemented in the [```DecisionTreeClassifier``` class](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) in scikit-learn.\n",
    "\n",
    "Parameters:\n",
    "- ```criterion```: ```'gini'``` (default) or ```'entropy'```\n",
    "- ```max_depth```: maximum depth of the tree (default: unbounded)\n",
    "- ```min_samples_leaf```: minimum number of examples in each leaf node (default 2)\n",
    "- and many more ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a427bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Train a Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(golf_preprocessed, golf_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604feb77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualising the tree\n",
    "\n",
    "As the decision tree classifier learns a model (other than the KNN classifier, which just stores all training data), we might be interested in looking at this model.\n",
    "\n",
    "You can explore the learned tree by using the [.tree](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html) member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea613adf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(dt,\n",
    "               feature_names=golf.columns, \n",
    "               class_names=label_encoder.classes_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdaa0ac",
   "metadata": {},
   "source": [
    "## Strengths of Decision Trees:\n",
    "- **Simplicity and Interpretability:** Decision trees are straightforward and easy to understand. You can visualize them (at least small-sized trees) like a flowchart, making it simple to see how decisions are made.\n",
    "- Works with **both numerical and categorical data**.\n",
    "- **Inexpensive** to construct and **fast** at classifying unknown records.\n",
    "- Requires **little data preparation**: no need to scale or normalize the features.\n",
    "- **Versatility**: They work for both classification and regression.\n",
    "\n",
    "## Limitations:\n",
    "- **Overfitting:** It can create over-complex trees that don't generalize well to new data; this can be mitigated with pruning and maximum depth constraints.\n",
    "- Decisions are based on **a single attribute at a time**.\n",
    "- Can only represent **decision boundaries parallel to the axes**.\n",
    "- **Biased towards dominant classes or features with many categories**.\n",
    "- **Instability:** The model can be unreliable, as slight variations in input can lead to significant differences in predictions.\n",
    "\n",
    "## ✅ Good Practice:\n",
    "✔ **Balance your dataset** before fitting the decision tree.\n",
    "\n",
    "✔ **Prevent overfitting** by using pruning, setting the minimum number of samples required at a leaf node, or setting the maximum depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ef66a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/random_forest_meme.jpg\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "## What is a Random Forest?\n",
    "A random forest is an ensemble model or meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve predictive accuracy and control overfitting.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/random_forest.png\" style=\"width: 70%;\">\n",
    "</div>\n",
    "\n",
    "### Random Forest in Python\n",
    "Common parameters to optimize:\n",
    "- **n_estimators** (The number of trees in the forest)\n",
    "- **criterion** {“gini”, “entropy”}\n",
    "- **max_depth**\n",
    "- **min_samples_split**\n",
    "- **min_samples_leaf**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19ab5a",
   "metadata": {},
   "source": [
    "### Random Forest for the Golf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbcf56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "random_forest.fit(golf_preprocessed, golf_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673d091",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/overfitting_meme.png\" style=\"width: 40%;\">\n",
    "</div>\n",
    "\n",
    "## What is Overfitting?\n",
    "The classifier has good accuracy on training data but performs poorly on test data because it fits too closely to the training data and fails to generalize to unseen examples.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/overfitting.png\" style=\"width: 60%;\">\n",
    "</div>\n",
    "\n",
    "### Signs of Overfitting\n",
    "- The model works well on the training set but performs poorly on the test set.\n",
    "- A decision tree that is too deep.\n",
    "- A decision tree with too many branches.\n",
    "\n",
    "### Causes of Overfitting\n",
    "- Noise/outliers in the training data.\n",
    "- Too little training data.\n",
    "- High model complexity.\n",
    "\n",
    "## ⚠️ **An overfitted model does not generalize well to unseen data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b21df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed679b2d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Why Can't We Use the Training Data to Measure How Well the Model Works?\n",
    "\n",
    "- We can measure a **training error**, which tells us how well the model represents the training dataset.\n",
    "- However, this does not tell us anything about the error on other data points!\n",
    "- Imagine we create a **1-NN classifier** – what would be the training error?  \n",
    "  → **Zero**, because each point is its own nearest neighbor! But this does not mean the model generalizes well.\n",
    "\n",
    "### For a correct measurement, we need a second dataset:\n",
    "- We can either **collect more data** to create a **test dataset**.\n",
    "- Or we **split the dataset** before training into two parts.\n",
    "\n",
    "We then evaluate the performance of our classification model by applying it to the **test dataset** (a *different* dataset) and measuring the error.\n",
    "\n",
    "## Train-Test Splitting\n",
    "\n",
    "Before training a model, we need to **split** our dataset into:\n",
    "- **Training Set** → Used to train the model.\n",
    "- **Test Set** → Used to evaluate model performance on unseen data.\n",
    "\n",
    "This prevents **overfitting**, where a model performs well on known data but fails on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e5925",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion Matrix\n",
    "It evaluates the predictive capability of a model in terms of the correctly/incorrectly classified instances.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/confusion_matrix.png\" style=\"width: 80%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b335fa5",
   "metadata": {},
   "source": [
    "### Confusion Matrix in Python\n",
    "\n",
    "#### Visualising the confusion matrix\n",
    "\n",
    "The following [sample code](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) visualises the confusion matrix for easier interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6c554",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## A helper function to plot the confusion matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4c792",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use the trained Decision Tree to predict the labels of the test set\n",
    "golf_prediction = dt.predict(golf_test)\n",
    "\n",
    "# Print the true target labels and the model's predictions\n",
    "display(golf_target_test)\n",
    "display(golf_prediction)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(golf_target_test, golf_prediction))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cnf_matrix = confusion_matrix(golf_target_test, golf_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd39c2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1️⃣ Accuracy\n",
    "\n",
    "A single measure that tells you the overall accuracy of the result\n",
    "    $$Accuracy=\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\text{Correct predictions}}{\\text{All predictions}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "display(golf_target_test)\n",
    "display(golf_prediction)\n",
    "accuracy = accuracy_score(golf_target_test, golf_test_prediction)\n",
    "\n",
    "# Please limit your results to a resonable decimal number \n",
    "print(f\"Accuracy: {accuracy*100} %\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab6493",
   "metadata": {},
   "source": [
    "### What is a good accuracy?\n",
    "✅ Depends on your problem!\n",
    "\n",
    "Baseline: naive guessing (e.g., always predict the majority class)\n",
    "\n",
    "\n",
    "### ⚠️ Limitations: Unbalanced data\n",
    "\n",
    "- If 99% belong to class “no disease”, and 1% to class \"disease\"\n",
    "- The classifier always says “no disease” – 99% Accuracy\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/accuracy_meme.jpeg\" style=\"width: 40%;\">\n",
    "</div>\n",
    "\n",
    "**Accuracy is misleading** because the model does not detect any positive examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c5cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  2️⃣ Precision, Recall, F1-score\n",
    "\n",
    "Measure two aspects of the result for every class\n",
    "\n",
    "### Precision: How many examples that  are classified  positive are actually positive?\n",
    "\n",
    "\n",
    "$Precision = \\frac{TP}{TP+\\textbf{FP}}$\n",
    "\n",
    "### Recall: Which fraction of all  positive examples is  classified correctly? \n",
    "\n",
    "$Recall = \\frac{TP}{TP+\\textbf{FN}}$\n",
    "\n",
    "### F1 score: considers both precision and recall (harmonic mean) \n",
    "\n",
    "$F1 = \\frac{2 * precicion * recall}{precision + recall}$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/precision_recall_matrix.png\" style=\"width: 80%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a7cea",
   "metadata": {},
   "source": [
    "### An example with 2 classes\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/confusion_matrix_example.png\" style=\"width: 80%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecaeac9",
   "metadata": {},
   "source": [
    "So far, the **$F_1$-measure** is only defined for individual classes and is not yet useful for getting an overview of the overall performance of a classifier. To address this, one commonly takes the average over all classes using one of the following two approaches:\n",
    "\n",
    "- **Micro Average $F_1$-Measure:**  \n",
    "  The values of **True Positives (TP), False Positives (FP),** and **False Negatives (FN)** are summed up across all classes. Then, **precision, recall,** and **$F_1$-measure** are computed using these totals.\n",
    "\n",
    "- **Macro Average $F_1$-Measure:**  \n",
    "  **Precision and recall** are computed separately for each class. Then, the **average precision** and **average recall** are used to calculate the final **$F_1$-measure**.\n",
    "\n",
    "These averaging methods help evaluate the overall classifier performance, especially when dealing with **imbalanced datasets**, where some classes may have significantly more instances than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf8c6a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(golf_target_test, golf_prediction, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3bcb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3️⃣ ROC Curves & AUC\n",
    "\n",
    "To evaluate and compare different classifiers, it can be useful to look at **ROC Curves**.\n",
    "\n",
    "### Receiver Operating Characteristics (ROC) Curves\n",
    "- A graphical way to evaluate how well a classification model performs, especially when distinguishing between two classes.\n",
    "- When your model makes a prediction, it assigns **probabilities** to each class (i.e., how certain the classifier is about its prediction).\n",
    "- ROC Curves visualize the **true positive rate (TPR)** against the **false positive rate (FPR)** in relation to the model's confidence.\n",
    "  \n",
    "    - **True Positive Rate (TPR) = Recall = Sensitivity**: Measures how well the model identifies actual positives.\n",
    "\n",
    "      $ TPR = \\frac{TP}{TP + FN} $\n",
    "\n",
    "    - **False Positive Rate (FPR)**: Measures how well the model identifies actual negatives.\n",
    "\n",
    "      $ FPR = \\frac{FP}{FP + TN} $\n",
    "\n",
    "### How to Create ROC Curves?\n",
    "1. Sort classifications according to their confidence scores.\n",
    "2. Evaluate:\n",
    "    - **Correct prediction** → Move one step up.\n",
    "    - **Incorrect prediction** → Move one step to the right.\n",
    "\n",
    "### How to Interpret ROC Curves?\n",
    "\n",
    "✅ **The steeper, the better**\n",
    "- Random guessing results in a diagonal line ($TPR = FPR$).\n",
    "- A good classifier should result in a curve **significantly above the diagonal**.\n",
    "- **Best possible results**: All correct predictions have higher confidence than all incorrect ones.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/roc_curves_auc.png\" style=\"width: 80%;\">\n",
    "</div>\n",
    "\n",
    "### AUC: Area Under the Curve\n",
    "- A single number that summarizes the performance of a classifier by telling us **how well the model distinguishes between two classes**.\n",
    "    - You can think of it as the **probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example** by the model.\n",
    "\n",
    "- **Hint**: It's the area under the ROC Curve.\n",
    "- **Interpretation**: AUC scores range between **0.0 and 1.0**:\n",
    "    - **$AUC = 1.0$** → Perfect classifier (always correct).\n",
    "    - **$AUC = 0.5$** → Random guessing (useless model).\n",
    "    - **$AUC < 0.5$** → Worse than random (model classifies incorrectly more often than correctly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60958de2",
   "metadata": {},
   "source": [
    "### ROC Curves in Python\n",
    "\n",
    "You can create these curves using the [`roc_curve()` function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html).  \n",
    "Since ROC Curves depend on the confidence scores of each prediction made by the classifier, you need to call the `predict_proba()` function instead of `predict()`.  \n",
    "\n",
    "- **`predict_proba()`** returns the confidence values for each class.\n",
    "- **`predict()`** returns the actual class prediction.\n",
    "\n",
    "This ensures that the model's probability estimates are used to plot the curve rather than just the final classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fafcb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create a kNN classifier\n",
    "knn_estimator = KNeighborsClassifier(3)\n",
    "\n",
    "# fit kNN to the training data\n",
    "knn_estimator.fit(golf_preprocessed, golf_target)\n",
    "\n",
    "# get the class probabilities (confidences) for the test set for kNN\n",
    "proba_for_each_class_knn = knn_estimator.predict_proba(golf_test)\n",
    "\n",
    "# get the class probabilities (confidences) for the test set for DT\n",
    "proba_for_each_class_dt = dt.predict_proba(golf_test)\n",
    "\n",
    "# calculate the ROC Curve for the kNN\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(golf_target_test, proba_for_each_class_knn[:,1], pos_label=1)\n",
    "\n",
    "# calculate the ROC Curve for the DT\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(golf_target_test, proba_for_each_class_dt[:,1], pos_label=1)\n",
    "\n",
    "# plot the line for random guessing\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random', alpha=.8)\n",
    "\n",
    "# plot the ROC Curve for the classifiers\n",
    "plt.plot(fpr_knn,tpr_knn,label='K-NN')\n",
    "plt.plot(fpr_dt,tpr_dt,label='Decision Tree')\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7b948",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's compute evaluation metrics for a multi-class classification problem (step-by-step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c83d15",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Given a data set $D = {o_1, . . . , o_n}$ with known class labels $C(o_i) ∈ C = \\{A, B, C\\}$ of the objects, we use a classifier $K$ to predict the class $K(o_i)$ of each object $o_i \\in D$. The table below gives shows the actual and the preedicted labels:\n",
    "\n",
    "| $i$ | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| $C(o_i)$ | A | B | A | C | C | B | A | A | A | B | B | C | C | C | B |\n",
    "| $K(o_i)$ | A | A | C | C | B | B | A | A | A | C | A | A | C | C | B |\n",
    "\n",
    "\n",
    "<sub>Source: https://www.dbs.ifi.lmu.de/Lehre/KDD/WS1819/tutorials/solution_10.pdf</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ac40f",
   "metadata": {},
   "source": [
    "#### 1. Setup the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877add05",
   "metadata": {},
   "source": [
    "#### 2. Compute the accuracy / classification error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae27d9",
   "metadata": {},
   "source": [
    "#### 3. Compute the precision and recall for each class $i \\in C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ead4a9",
   "metadata": {},
   "source": [
    "#### 4. Compute the $F_1$-measure for all classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af331ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 5. Compute the Micro- and Macro-Average $F_1$-measures for the example above. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551bcde",
   "metadata": {},
   "source": [
    "# QUIZ TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182da33",
   "metadata": {},
   "source": [
    "## Question 1:  \n",
    "Consider a scenario where a dataset contains two classes that are **not linearly separable** but have clear clusters. Would a nearest centroid classifier work well? Why or why not?  a **non-linear** boundary that separates the clusters correctly."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "not because the labble can not seperate",
   "id": "bf9f71e054469247"
  },
  {
   "cell_type": "markdown",
   "id": "f33117fc",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "\n",
    "A decision tree has grown very deep and performs perfectly on the training set. However, it performs poorly on unseen test data. **What two main techniques can be applied to improve generalization, and how do they work?**a"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "use instead a esembler model or pruning with max depth, so can limited the growth",
   "id": "9df83e4b7d75122f"
  },
  {
   "cell_type": "markdown",
   "id": "cfc6b344",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "\n",
    "You are given a dataset with **only 20 training samples but 500 features**. What issue do you expect when training a decision tree, and how can you address it?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "it would overfitting, use feature seletcion with pca (lower teh deimen",
   "id": "7d61449de738a5b"
  },
  {
   "cell_type": "markdown",
   "id": "0a386ae0",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "\n",
    "A model has **high precision but low recall**. In what kind of real-world application might this tradeoff be acceptable? Conversely, when would **high recall but low precision** be preferable?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "e.g bankfraud it worse to not detect fraud as sign valid transaction as fraud",
   "id": "c16de3bc4ab97548"
  },
  {
   "cell_type": "markdown",
   "id": "3ea36339",
   "metadata": {},
   "source": [
    "## Question / Task 5  \n",
    "\n",
    "Using the **Iris dataset**, train:  \n",
    "(i) a **Decision Tree classifier**  \n",
    "(ii) a **Random Forest classifier**  \n",
    "\n",
    "1. **Plot the decision tree.**  \n",
    "2. **Evaluate the accuracy** of both models and **draw a confusion matrix** for each.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
